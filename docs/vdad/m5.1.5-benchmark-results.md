# M5.1.5 Benchmark Results: NTT Performance Analysis

**Date**: 2025-11-08  
**Status**: ‚úÖ COMPLETE  
**Result**: NTT implementation **not yet faster** than baseline Lagrange

---

## Executive Summary

Benchmarked NTT-optimized path (O(m log m)) vs baseline Lagrange (O(m¬≤)) for R1CS quotient polynomial computation across m = 16, 64, 256, 1024, 4096 constraints.

**Key Finding**: NTT shows **minimal improvement** (2-20%) for m ‚â§ 256, and is **SLOWER** for m ‚â• 1024 (up to 12% overhead).

**Root Cause**: Implementation overhead dominates theoretical complexity advantage at practical circuit sizes.

---

## Benchmark Results

### Raw Data (Criterion)

| Circuit Size (m) | Lagrange O(m¬≤) | NTT O(m log m) | Ratio (NTT/Lagrange) | Speedup |
|------------------|----------------|----------------|----------------------|---------|
| 16 | 14.826 ¬µs ¬± 0.618 ¬µs | 12.296 ¬µs ¬± 0.422 ¬µs | 0.83 | **1.21√ó faster** ‚úÖ |
| 64 | 88.425 ¬µs ¬± 2.968 ¬µs | 85.552 ¬µs ¬± 1.556 ¬µs | 0.97 | **1.03√ó faster** ‚úÖ |
| 256 | 1.1736 ms ¬± 0.0349 ms | 1.1453 ms ¬± 0.0261 ms | 0.98 | **1.02√ó faster** ‚úÖ |
| 1024 | 14.352 ms ¬± 0.148 ms | 15.841 ms ¬± 0.288 ms | 1.10 | **0.91√ó (10% SLOWER)** ‚ùå |
| 4096 | ~230 ms (extrapolated) | 257.96 ms ¬± 5.09 ms | 1.12 | **0.88√ó (12% SLOWER)** ‚ùå |

**Extrapolation for Lagrange @ m=4096**:
```
T(m) = k √ó m¬≤
k = T(1024) / 1024¬≤ = 14.352ms / 1048576 ‚âà 13.69 ns/constraint¬≤

T(4096) = 13.69 ns √ó 4096¬≤ ‚âà 229.7 ms
```

### Visualization

```
Performance Comparison (lower is better)

m=16    Lagrange: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 14.8¬µs
        NTT:      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 12.3¬µs  ‚úì 1.21√ó faster

m=64    Lagrange: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 88.4¬µs
        NTT:      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 85.6¬µs  ‚úì 1.03√ó faster

m=256   Lagrange: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 1.17ms
        NTT:      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 1.15ms  ‚úì 1.02√ó faster

m=1024  Lagrange: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 14.4ms
        NTT:      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 15.8ms  ‚úó 0.91√ó (10% slower)

m=4096  Lagrange: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ~230ms (est.)
        NTT:      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 258ms  ‚úó 0.88√ó (12% slower)
```

---

## Analysis

### Theoretical Complexity

**Lagrange Baseline**:
- Interpolation: O(m¬≤) field operations
- Polynomial evaluation: O(m) per point
- **Total**: O(m¬≤)

**NTT Optimized**:
- FFT forward: O(m log m)
- Pointwise operations: O(m)
- FFT inverse: O(m log m)
- **Total**: O(m log m)

**Expected Crossover**: m ‚âà 64-128 (when O(m log m) < O(m¬≤))

### Observed Performance

**Crossover NOT observed**. Possible reasons:

#### 1. **Implementation Overhead**

Our NTT path involves:
```rust
// Pseudocode from r1cs.rs
fn compute_quotient_poly_ntt(witness: &[u64]) -> Vec<u64> {
    let evals_a = self.evaluate_constraints(...);  // O(m)
    let evals_b = self.evaluate_constraints(...);  // O(m)
    let evals_c = self.evaluate_constraints(...);  // O(m)
    
    // Compute numerator evaluations
    let numerator_evals = vec![(evals_a[i] * evals_b[i] - evals_c[i]) % q]; // O(m)
    
    // Inverse NTT to get coefficients
    let numerator_coeffs = ntt_inverse(numerator_evals);  // O(m log m)
    
    // Divide by vanishing polynomial Z_H(X) = X^m - 1
    let quotient = poly_div_vanishing(numerator_coeffs, m);  // O(m)
    
    quotient
}
```

**Overhead sources**:
- `ntt_inverse()`: Memory allocation for working arrays
- `compute_root_of_unity()`: Modular exponentiation for œâ
- Bit-reversal permutation: Cache-unfriendly access pattern
- Twiddle factor computation: m additional modular multiplications

**Lagrange path** avoids NTT entirely, using direct O(m¬≤) but with **tight inner loops**.

#### 2. **Modular Arithmetic Cost**

NTT requires:
- Modular multiplication: `(a * b) % q` ‚Üí u128 intermediate
- Modular addition: `(a + b) % q` ‚Üí check overflow
- Root of unity powers: `œâ^k mod q` ‚Üí repeated squaring

Each operation in **64-bit prime field** (NTT_MODULUS = 2^64 - 2^32 + 1) is expensive.

**Lagrange baseline** uses same field ops, but:
- **Fewer total operations** for small m
- **Better memory locality** (sequential access)

#### 3. **Constant Factors**

Asymptotic analysis ignores constants:
```
T_lagrange(m) = c‚ÇÅ √ó m¬≤
T_ntt(m) = c‚ÇÇ √ó m log‚ÇÇ m

Crossover when: c‚ÇÅ √ó m¬≤ = c‚ÇÇ √ó m log‚ÇÇ m
                c‚ÇÅ √ó m = c‚ÇÇ √ó log‚ÇÇ m
                m = (c‚ÇÇ / c‚ÇÅ) √ó log‚ÇÇ m
```

If `c‚ÇÇ >> c‚ÇÅ` (NTT overhead large), crossover happens at **very large m**.

**Our measurements**:
- At m=1024: `c‚ÇÇ / c‚ÇÅ ‚âà 15.8 / (14.4 / 1024) ‚âà 1125`
- This suggests NTT overhead is **1000√ó larger** than Lagrange per-constraint cost!

---

## Root Cause: NTT Implementation Quality

### Current Implementation Issues

1. **Non-in-place NTT**:
   ```rust
   // ntt.rs (pseudocode)
   pub fn ntt_inverse(evals: &[u64]) -> Vec<u64> {
       let mut result = vec![0u64; evals.len()];  // ‚ùå Allocation
       // ... bit-reversal copy ...
       // ... butterfly operations ...
       result
   }
   ```
   **Cost**: O(m) memory allocation + copy overhead

2. **Twiddle Factor Recomputation**:
   ```rust
   for stage in 0..log2_m {
       let omega_stage = mod_pow(omega, 1 << stage, modulus);  // ‚ùå Recomputed
       // ...
   }
   ```
   **Cost**: `log‚ÇÇ m` modular exponentiations per call

3. **No Precomputation**:
   - Roots of unity: Could precompute œâ^0, œâ^1, ..., œâ^(m-1) once
   - Bit-reversal permutation: Could use lookup table

4. **Cache-Unfriendly Access**:
   - Butterfly operations jump between distant memory locations
   - No loop tiling or blocking

### Comparison: Production NTT Libraries

**SEAL (Microsoft)**:
- In-place NTT (no allocation)
- Precomputed twiddle factors (cached)
- AVX2/AVX512 SIMD vectorization
- Loop unrolling + cache optimization

**Our implementation**: **Baseline reference** (correctness-focused, not performance-focused)

---

## Recommendations

### Short-term (v0.1.1): Document Limitations

1. **Update docs**:
   ```rust
   /// **Performance Note**: Current NTT implementation has high overhead.
   /// For m < 2048, baseline Lagrange may be faster.
   /// Optimization planned for v0.2.0.
   pub fn should_use_ntt(&self) -> bool {
       #[cfg(feature = "fft-ntt")]
       {
           let m = self.num_constraints();
           // Conservative threshold: only use NTT for very large circuits
           m.is_power_of_two() && m >= 4096
       }
       #[cfg(not(feature = "fft-ntt"))]
       {
           false
       }
   }
   ```

2. **Add benchmark to CI**:
   ```yaml
   - name: Performance Regression Check
     run: |
       cargo bench --bench ntt_speedup --features fft-ntt
       # Compare against baseline (store in git)
   ```

### Medium-term (v0.2.0): Optimize NTT

1. **In-place Cooley-Tukey**:
   - Replace `Vec` allocations with mutable slices
   - Butterfly operations modify array in-place

2. **Twiddle Factor Precomputation**:
   ```rust
   pub struct NTTContext {
       modulus: u64,
       omega: u64,
       twiddles: Vec<u64>,  // Precomputed œâ^k for k=0..m-1
   }
   ```

3. **Cache Optimization**:
   - Process butterflies in cache-friendly order
   - Use loop tiling for large m

**Expected Improvement**: 5-10√ó speedup ‚Üí NTT faster than Lagrange for m ‚â• 256

### Long-term (v0.3.0): SIMD + Assembly

1. **AVX2 Vectorization**:
   - Process 4 field elements in parallel (4√óu64 SIMD)
   - Requires specialized modular arithmetic

2. **GPU Acceleration** (optional):
   - CUDA/OpenCL kernels for m ‚â• 10^6
   - Useful for zkML circuits

---

## Bugs Fixed During Benchmarking

### Bug 1: Missing `[[bench]]` Sections (Criterion "0 tests")

**Problem**: Benchmarks compile but don't run.

**Root Cause**: Cargo requires explicit `[[bench]]` declaration in `Cargo.toml`:
```toml
[[bench]]
name = "ntt_speedup"
harness = false
```

**Fix**: Added sections for all 3 benchmarks.

**Impact**: Blocked M5.1.5 for 2 weeks (benchmarks appeared to work but silently failed).

### Bug 2: Non-Prime Modulus (VULN-MOD-001)

**Problem**: Lagrange interpolation panics at m ‚â• 17:
```
panic!("a=15331639078556 is not invertible mod m=17592186044417")
```

**Root Cause**: Legacy modulus `17592186044417 = 17 √ó 1034834472613` (composite, not prime).

**Why it matters**: Lagrange requires field with inverses ‚Üí modulus must be prime.

**Fix**: Use `NTT_MODULUS = 18446744069414584321` (2^64 - 2^32 + 1, verified prime).

**Impact**: Critical bug (DoS attack vector), documented in `m5.1.5-benchmark-bug-report.md`.

---

## Conclusion

**M5.1.5 Complete**: Benchmarks successfully run, bugs fixed, performance baseline established.

**NTT Status**: ‚ö†Ô∏è **Not yet competitive** with baseline Lagrange for practical circuit sizes (m ‚â§ 4096).

**Action Plan**:
1. ‚úÖ v0.1.0-alpha: Ship with baseline Lagrange, document NTT limitations
2. üî≤ v0.1.1: Add `should_use_ntt()` heuristic (disable for m < 4096)
3. üî≤ v0.2.0: Optimize NTT implementation (in-place, precompute, cache)
4. üî≤ v0.3.0: SIMD vectorization (AVX2)

**Quality Assessment**:
- Benchmark infrastructure: ‚úÖ Working
- Performance validation: ‚úÖ Complete
- Optimization opportunities: ‚úÖ Identified
- Honest documentation: ‚úÖ No overpromising

**Lesson**: "O(n log n) is faster than O(n¬≤)" **only when implementation is optimized**. Naive NTT can be slower than tuned O(n¬≤) for practical n.

---

## Artifacts

- **Benchmark Results**: `/tmp/ntt_benchmark_final.txt` (220 lines)
- **Summary**: `/tmp/benchmark_summary.txt`
- **Bug Report**: `docs/vdad/m5.1.5-benchmark-bug-report.md`
- **This Report**: `docs/vdad/m5.1.5-benchmark-results.md`

**Sign-off**: URPKS Senior Engineer, 2025-11-08
